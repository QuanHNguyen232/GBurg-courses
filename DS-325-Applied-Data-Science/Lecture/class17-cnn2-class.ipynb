{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class - Convolutional Neural Networks 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, you will load MobileNetV2: use it to predict the class of the image of your choice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "#\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import  preprocess_input\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "#\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create the base model from the MobileNet V2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and book store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Returns an image with shape [height, width, num_channels].\"\"\"\n",
    "    response = requests.get(url)\n",
    "    image    = Image.open(BytesIO(response.content))\n",
    "    image    = np.array(image)\n",
    "    return image\n",
    "\n",
    "def show_image(image,ax=None):\n",
    "    if ax==None: ax=plt.gca()\n",
    "    ax.imshow(image);\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 4 images located on the web to predict the class label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image \n",
    "img_url1 = 'http://public.gettysburg.edu/~jpuckett/ds325/img/laney.jpg'\n",
    "img_url2 = 'http://public.gettysburg.edu/~jpuckett/ds325/img/togo.jpg'\n",
    "img_url3 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Cherry_blossom_flowers_1.jpg/724px-Cherry_blossom_flowers_1.jpg'\n",
    "img_url4 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Ring-tailed_lemur_%28Lemur_catta%29.jpg/683px-Ring-tailed_lemur_%28Lemur_catta%29.jpg'\n",
    "\n",
    "#\n",
    "img1     = load_image_from_url(img_url1)\n",
    "img2     = load_image_from_url(img_url2)\n",
    "img3     = load_image_from_url(img_url3)\n",
    "img4     = load_image_from_url(img_url4)\n",
    "show_image(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the base model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =tf.keras.applications.MobileNetV2( include_top=True, weights='imagenet',input_shape=(224,224,3))\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model expects the image to be in the batch format (#batch,img_width,img_height,#channels)\n",
    "* The input need to be pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process an image to be mobilenet friendly\n",
    "def process_image(img,IMG_SIZE):\n",
    "    imgR              = tf.image.resize(img,(IMG_SIZE,IMG_SIZE))    \n",
    "    img_array         = np.array(imgR)\n",
    "    img_array         = np.expand_dims(img_array, axis=0)\n",
    "    pImg              = preprocess_input(img_array)\n",
    "    return pImg\n",
    "def classify_images(base_model,img,IMG_SIZE=224):\n",
    "    #preprocess image\n",
    "    imgA              = process_image(img,IMG_SIZE)\n",
    "    prediction        = base_model.predict(imgA)\n",
    "    #output from model to determin class\n",
    "    top3              = decode_predictions(prediction, top=3)[0]\n",
    "    return top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = classify_images(base_model,img1)\n",
    "print(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put it all together to show the image and get the label\n",
    "def imshowCaption(base_model,img,ax=None):\n",
    "    top3 = classify_images(base_model,img)\n",
    "    if ax is None: ax=plt.gca()\n",
    "    show_image(img)\n",
    "    #make top choice str\n",
    "    lbl = top3[0][1]\n",
    "    acc = top3[0][2]    \n",
    "    str1 = '%s with accuracy=%2.1f%%'%(lbl,acc*100)\n",
    "    #make 2nd choice str\n",
    "    lbl = top3[1][1]\n",
    "    acc = top3[1][2]    \n",
    "    str2 = '%s with accuracy=%2.1f%%'%(lbl,acc*100)\n",
    "    lbl = top3[2][1]\n",
    "    acc = top3[2][2]    \n",
    "    str3 = '%s with accuracy=%2.1f%%'%(lbl,acc*100)\n",
    "    \n",
    "    plt.title('%s\\n%s\\n%s'%(str1,str2,str3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshowCaption(base_model,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax1=plt.subplot(221)\n",
    "imshowCaption(base_model,img1,ax1)\n",
    "ax2=plt.subplot(222)\n",
    "imshowCaption(base_model,img2,ax2)\n",
    "ax3=plt.subplot(223)\n",
    "imshowCaption(base_model,img3,ax3)\n",
    "ax4=plt.subplot(224)\n",
    "imshowCaption(base_model,img4,ax4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 seems to be classifying the images with great accuracy. \n",
    "\n",
    "\n",
    "* Togo is mislabeled, should be 'Eskimo dog, husky'.  'kelpie' is an 'Australian kelpie', not the mythological creature.\n",
    "* The 'madagascar_cat' is otherwise known as a ring-taled lemur.\n",
    "* It was not very good at identifying the cherry blossoms.  There are surprisingly few flower species in the imagenet dataset.  We'll get to that below.\n",
    "\n",
    "While MobileNetV2 did a great job with these, one of your hw problems is for you to test a few images yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "\n",
    "Transfer learning involves taking a pre-trained model, extracting one of the layers, then taking that as the input layer to a series of dense layers. This pre-trained model is usually trained by institutions or companies that have much larger computation and financial resources. Some of these popular trained models for image recognition tasks are VGG, Inception, ResNet and MobileNet.\n",
    "\n",
    "Using this newly formed model, we can then set the parameters within the pre-trained model to be non-trainable while only optimizing the parameters of the subsequent dense layers during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(test_set_raw, valid_set_raw, train_set_raw),info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True,with_info=True)\n",
    "class_names   = info.features[\"label\"].names\n",
    "n_classes     = info.features[\"label\"].num_classes\n",
    "dataset_size  = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label,IMG_SHAPE=224):\n",
    "    resized_image = tf.image.resize(image, [IMG_SHAPE, IMG_SHAPE])\n",
    "    final_image = tf.keras.applications.mobilenet_v2.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = train_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set  = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE  = 224\n",
    "base_model2 = tf.keras.applications.MobileNetV2(input_shape=(IMG_SHAPE,IMG_SHAPE,3),\n",
    "                                              include_top=False, \n",
    "                                              weights='imagenet')\n",
    "for layer in base_model2.layers:\n",
    "    layer.trainable = False\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model2.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "#now construct our model from these layers, we could add a hidden layer, but lets try something simple first\n",
    "model = tf.keras.models.Model(inputs=base_model2.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(base_model2.layers):\n",
    "    print(index, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model.  \n",
    "#### Note: on my laptop, this takes about 10s/epoch\n",
    "* We are only using 1/10th the data for each epoch.  Since we are more interested in the concept than the optimally trained model.\n",
    "* This gives us better visualization of how the model is learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs   = 6\n",
    "tstart   = tf.timestamp()\n",
    "history  = model.fit(train_set,\n",
    "                    steps_per_epoch =int(0.1 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.05 * dataset_size / batch_size),\n",
    "                    epochs=epochs)\n",
    "total_time = tf.timestamp() - tstart\n",
    "print(\"total time %3.3f seconds\"%total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAccuracy(history,results_test):\n",
    "    print(\"train loss %.5f \\t train acc: %.5f\"%(history.history['loss'][-1],history.history['accuracy'][-1]))\n",
    "    print(\"valid loss %.5f \\t valid acc: %.5f\"%(history.history['val_loss'][-1],history.history['val_accuracy'][-1]))\n",
    "    print(\"test loss  %.5f \\t test acc:  %.5f\"%(results_test[0],results_test[1]))\n",
    "def plot_result(history,results_test):\n",
    "    # Get training and validation histories\n",
    "    training_acc = history.history['accuracy']\n",
    "    val_acc      = history.history['val_accuracy']\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_acc) + 1)\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_acc, 'b-o',label='Training')\n",
    "    plt.plot(epoch_count, val_acc, 'r--',label='Validation')\n",
    "    plt.plot(epoch_count, results_test[1]*np.ones(len(epoch_count)),'k--',label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===    \n",
    "results_test = model.evaluate(test_set, batch_size=128,verbose=0)    \n",
    "printAccuracy(history,results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(history,results_test)   \n",
    "plt.title(\"Transfer learning using MobileNetV2, acc=%2.3f, in %3.2f s\"%(results_test[1],total_time)) #overwrite the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like we are overtraining.  85% is remarkable given <2min of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
