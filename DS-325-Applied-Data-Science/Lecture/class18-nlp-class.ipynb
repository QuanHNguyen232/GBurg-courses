{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class - Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "# Natural Language Toolkit \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#-----------------------------\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests #web\n",
    "from collections import Counter #counting words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string \n",
    "import re\n",
    "#-----------------------------\n",
    "# other libraries we might use\n",
    "#import string\n",
    "\n",
    "#-----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (9.71,6)\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns; \n",
    "sns.axes_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple search and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter. That book was made by Mr. Mark Twain, and he told the truth, mainly. There was things which he stretched, but mainly he told the truth. That is nothing. I never seen anybody but lied one time or another, without it was Aunt Polly, or the widow, or maybe Mary. Aunt Polly--Tom's Aunt Polly, she is--and Mary, and the Widow Douglas is all told about in that book, which is mostly a true book, with some stretchers, as I said before.\\nNow the way that the book winds up is this: Tom and me found the money that the robbers hid in the cave, and it made us rich. We got six thousand dollars apiece--all gold. It was an awful sight of money when it was piled up. Well, Judge Thatcher he took it and put it out at interest, and it fetched us a dollar a day apiece all the year round--more than a body could tell what to do with. The Widow Douglas she took me for her son, and allowed she would sivilize me; but it was rough living in the house all the time, considering how dismal regular and decent the widow was in all her ways; and so when I couldn't stand it no longer I lit out. I got into my old rags and my sugar-hogshead again, and was free and satisfied. But Tom Sawyer he hunted me up and said he was going to start a band of robbers, and I might join if I would go back to the widow and be respectable. So I went back.\\nThe widow she cried over me, and called me a poor lost lamb, and she called me a lot of other names, too, but she never meant no harm by it. She put me in them new clothes again, and I couldn't do nothing but sweat and sweat, and feel all cramped up. Well, then, the old thing commenced again. The widow rung a bell for supper, and you had to come to time. When you got to the table you couldn't go right to eating, but you had to wait for the widow to tuck down her head and grumble a little over the victuals, though there warn't really anything the matter with them,--that is, nothing only everything was cooked by itself. In a barrel of odds and ends it is different; things get mixed up, and the juice kind of swaps around, and the things go better.\"\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r'things', raw_text, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 3 of the regular expression matches from Exercise 7, using re.findall(). \n",
    "\n",
    "If you are using \"()\" in your regular expression, make sure to put a pair of parenthesis around the entire expression so the result includes the entire string match. \n",
    "1. Using re.findall(), formulate a regular expression that matches all capitalized words (\"You\",\"Tom\", \"Sawyer\",\"Aunt\",\"Polly\", etc.). \n",
    "2. Count the number of times each of the following words are used:\n",
    "* Polly\n",
    "* the\n",
    "3. Using re.sub(), replace all instances of the word \"you\" with \"I\".\n",
    "4. Tokenize the text using white space with regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find all capitalized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitalized = re.findall('([A-Z][a-z]+)', raw_text)\n",
    "print(capitalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Count the number of times each of the following words are used:\n",
    "* Polly\n",
    "* the\n",
    "* words ending with \"-ed\"\n",
    "\n",
    "\n",
    "| metacharacter | description |\n",
    "| - | :- |\n",
    "| \\d | Whole Number 0 - 9 | \n",
    "| \\w | Used to find a word character. A word character is a character from a-z, A-Z, 0-9, including the _ (underscore) character. | \n",
    "| \\b | Used to find a match at the beginning or end of a word. | \n",
    "| [0-9]  | Used to find any character between the brackets. |\n",
    "| [a-z]  | Used to find any character between the brackets. |\n",
    "| [A-z ] | Any character from uppercase A to lowercase z |\n",
    "| (x\\|y) | Used to find any of the alternatives specified |\n",
    "| * | Used to match 0 or more of the previous (e.g. xy*z could correspond to \"xz\", \"xyz\", \"xyyz\", etc.) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPolly = len(re.findall(\"Polly\", raw_text))                  #don't want to ignore case\n",
    "numThe   = len(re.findall(\"the\", raw_text,   re.IGNORECASE))\n",
    "numIng   = len(re.findall(r'\\b(\\w+ing)\\b',   raw_text))        #find -ing words\n",
    "print(\"Polly counted       : %d\"%numPolly)\n",
    "print(\"The counted         : %d\"%numThe)\n",
    "print(\"-ing word counted   : %d\"%numIng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Produce a list of 'so ...ly' phrases, that is, 'so' modifying a word ending in 'ly'. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listLY   = set(re.findall(r'\\b(\\w+ly)\\b',   raw_text))     #\\w any characters +  ly anchored at end of word\n",
    "print(\"-ly words       : \")\n",
    "print(listLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using re.sub(), replace all instances of the word \"you\" with \"I\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = re.sub('You', 'I', raw_text)     #note that 'you' is not replaced\n",
    "print(out1[:50])   \n",
    "out2 = re.sub('[Y,y]ou', 'I', raw_text) #both 'you' and 'You' replaced ### BEST\n",
    "print(out2[:50])    \n",
    "out3 = raw_text.replace('You', 'I')     #alternative use '.replace', doesn't handle lower/upper case\n",
    "print(out3[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tokenize the text using white space with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = re.split(' ',raw_text)\n",
    "print(out4[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Natural language toolkit\n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "Once the text has been segmented into its tokens (paragraphs,\n",
    "sentences, words), most NLP pipelines do a number of other basic\n",
    "procedures for text normalization, e.g.:\n",
    "* Tokenizing \n",
    "* Part of speech tagging\n",
    "* Lowercasing\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Stopword removal\n",
    "* TFIDF vectorization (term frequency–inverse document frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start simple\n",
    "\n",
    "This process of segmenting a string of characters into words is known as tokenization. Tokenization is a prelude to pretty much everything else we might want to do in NLP, since it tells our processing software what our basic units are. We will discuss tokenization in more detail shortly.\n",
    "\n",
    "We also pointed out that we could compile a list of the unique vocabulary items in a string by using set() to eliminate duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"One morning I shot an elephant in my pajamas. How he got into my pajamas, I'll never know.\" #groucho marx\n",
    "words = data.split() #default is to split on ' ' whitespace\n",
    "num_words  = len(words)\n",
    "num_unique = len(set(words))\n",
    "print(\"Number of words        : %d\"%num_words)\n",
    "print(\"Number of unique words : %d\"%num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its only tokenizing based on whitespace. There's not regard for punctuation, capitalization, apostrophes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text from gutenberg\n",
    "\n",
    "We will be loading **Dracula** by Bram Stoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dracula=\"http://www.gutenberg.org/cache/epub/345/pg345.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(url):\n",
    "    response = requests.get(url)\n",
    "    full_text = response.content\n",
    "    raw = full_text.decode(\"utf-8-sig\")    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text= load_book(url_dracula)\n",
    "print(raw_text[:505])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Extract words from text using NLP\n",
    "You'll now use nltk, the Natural Language Toolkit, to\n",
    "\n",
    "Tokenize the text (fancy term for splitting into tokens, such as words);\n",
    "Remove stopwords (words such as 'a' and 'the' that occur a great deal in ~ nearly all English language texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw_text) #better than just using regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else.\n",
    "\n",
    "Sometimes it may be easiest to grab a first part of the text (enough words to be unique) and search for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_text.find(\"JONATHAN HARKER\\'S JOURNAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text.rfind(\"THE END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = raw_text[4115:861589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "You want to tokenize your text, that is, split it into a list a words.\n",
    "\n",
    "Tokenization is the process of breaking raw text into its building\n",
    "parts: words, phrases, symbols, or other meaningful elements\n",
    "called tokens.\n",
    "A list of tokens is almost always the first step to any other NLP\n",
    "task.\n",
    "\n",
    "\n",
    "There are many tokenizers to choose from: sent_tokenize, word_tokenize, TreebankWordTokenizer, etc.\n",
    "\n",
    "First let's tokenize by **sentence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sent = nltk.tokenize.sent_tokenize(text)\n",
    "text_sent[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also construct our own tokenizer using regular expressions, or regex.\n",
    "\n",
    "Here we just split the text using **whitespace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "# Create tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also wish to use the 'word_tokenizer' built-in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "tokens = tokenizer(text)\n",
    "print(tokens[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging\n",
    "\n",
    "Let's tag the Parts of Speech to each of the words.\n",
    "\n",
    "POS tagging does not always provide the same label for a given word, but decides on the correct label for the specific context – disambiguates across the word classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(tokens)\n",
    "print(tagged_words[:55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it has correctly tagged the tokens.  I'll list a few of these below:\n",
    "* CC – coordinating conjunction\n",
    "* RB – adverb\n",
    "* IN – preposition\n",
    "* NN – noun\n",
    "* JJ – adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: if we aren't going to need these POS tags for later analysis, then we can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation and numbers\n",
    "\n",
    "punctuation is <code>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</code>\n",
    "\n",
    "Let's strip the punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_noPunct = [word for word in tokens if word[0].isalpha()]\n",
    "tokens_noPunct[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this has had the desired effect, mostly.  It now comes down to what we want out of the text.  The more we want to filter and tokenize, the more complex it can get from here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Case: lowercase\n",
    "It is common to convert all words to one case.\n",
    "\n",
    "This means that the vocabulary will shrink in size, but some distinctions are lost (e.g. “Apple” the company vs “apple” the fruit is a commonly used example).\n",
    "\n",
    "We can convert all words to lowercase by calling the lower() function on each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_lower = [word.lower() for word in tokens_noPunct]\n",
    "print(tokens_lower[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text is really hard, problem specific, and full of tradeoffs.\n",
    "\n",
    "Remember, simple is better.\n",
    "\n",
    "Simpler text data, simpler models, smaller vocabularies. You can always make things more complex later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency\n",
    "\n",
    "Without the NLTK package, creating a frequency distribution plot (histogram) for a BoW is possible, but will take multiple lines of code to do so. Through the use of the FreqDist class, we are able to obtain the frequencies of every token in the BoW with one single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency distribution\n",
    "fdist1 = nltk.FreqDist(tokens_lower)\n",
    "# Output top 10 words\n",
    "for word, frequency in fdist1.most_common(10):\n",
    "    print(u'{};{}'.format(word, frequency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in function to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fdist1.plot(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Most of these are stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n",
    "\n",
    "It is common practice to remove words that appear alot in the English language such as 'the', 'of' and 'a' (known as stopwords) because they're not so interesting. For more on all of these techniques, check out our Natural Language Processing Fundamentals in Python course.\n",
    "\n",
    "The package nltk has a list of stopwords in English which you'll now store it.\n",
    "\n",
    "If you get an error here, run the command \n",
    "<code>nltk.download('stopwords')</code>\n",
    "to install the stopwords on your system.\n",
    "\n",
    "\n",
    "For some applications like documentation classification, it may make sense to remove stop words. Others, maybe not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(stop_words))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_noStop = [w for w in tokens_lower if not w in stop_words]\n",
    "print(tokens_noStop[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the result with the stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate frequency distribution\n",
    "fdist2 = nltk.FreqDist(tokens_noStop )\n",
    "fdist2.plot(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to plot more words on a log scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sort word frequency distribution by number of times each word occurs\n",
    "sorted_counts = sorted(fdist2.items() , key = lambda x: x[1] ,reverse = True)\n",
    "x_vec         = [i+1 for i in range(len(sorted_counts))] # get rank of each word\n",
    "y_vec         = [freq for (word,freq) in sorted_counts]  # get count only\n",
    "plt.loglog(x_vec, y_vec)\n",
    "plt.xlabel('word rank')\n",
    "plt.ylabel('word counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What are some of the long words that appear in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove duplicate words \n",
    "word_bank     = set(tokens_noStop)\n",
    "#returns only words longer than 16 letters\n",
    "lengthy_words = [word for word in word_bank if len(word) > 15]\n",
    "#print the lengthy words\n",
    "print(lengthy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):  \n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return longest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_word  = find_longest_word(word_bank)\n",
    "print(longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure that counts as the longest word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sent = nltk.tokenize.sent_tokenize(text)\n",
    "longest = max([len(s) for s in text_sent])\n",
    "print(longest)\n",
    "print([s for s in text_sent if (len(s) == longest)]) #this is characters not words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Properties Table** Next there is a table of word properties, which you should compute (skip unique word stems, since we will do stemming in class on Wed).  Make a table that prints out:\n",
    "1. number of words\n",
    "2. number of unique words\n",
    "3. average word length\n",
    "4. longest word\n",
    "\n",
    "You can decide for yourself if you want to try this again after you eliminate punctuation and function words (stop words) or not.  It's your collection!  \n",
    "\n",
    "\n",
    "To get the average number of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_words_in_sentence(sentences):\n",
    "    counts = []\n",
    "    for sentence in sentences:\n",
    "        counts.append(len(sentence.split()))\n",
    "    return float(sum(counts))/len(counts) #number of words, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_length = sum(len(word) for word in tokens_noStop) / len(tokens_noStop)\n",
    "avg_sent_length = sum(len(sent) for sent in text_sent) / len(text_sent) #this counts characters, not words\n",
    "#unique words\n",
    "word_bank          = set(tokens_noStop)\n",
    "num_unique_words   = len(word_bank)\n",
    "avg_word_sent      = average_words_in_sentence(text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print each below\n",
    "print(\"Average sentence length     : %d   characters\"%avg_sent_length ) #in characters\n",
    "print(\"Average word length         : %3.2f characters\"%avg_word_length )#in characters\n",
    "print(\"Number of unique words      : %d\"%num_unique_words )             #\n",
    "print(\"Average words in sentence   : %3.2f\"%avg_word_sent )             #in words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming refers to the process of reducing each word to its root or base.\n",
    "\n",
    "For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "\n",
    "Some applications, like document classification, may benefit from stemming in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
    "\n",
    "There are many stemming algorithms, although a popular and long-standing method is the Porter Stemming algorithm. This method is available in NLTK via the PorterStemmer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = [porter.stem(word) for word in tokens_noStop]\n",
    "print(tokens_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that words have been reduced to their stems, such as \n",
    "* “arrive” has become “arriv“\n",
    "* “early” has become “earli“ \n",
    "* “morning” has become “morn“ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization\n",
    "\n",
    "Stemming can often create non-existent words, whereas lemmas are actual words\n",
    "\n",
    "We can use NLTK WordNet Lemmatizer uses the WordNet Database to lookup lemmas, however, lemmatizing requires the use of POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "  \n",
    "print(\"rocks :\", wnl.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", wnl.lemmatize(\"corpora\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", wnl.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "For instance:\n",
    "\n",
    "The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word vectors, counting words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['dracula','bat','fool','red','pride','sense']\n",
    "for w in word_list:\n",
    "    print(\"%s \\t\\t: %d\"%(w,fdist1[w]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Let's now compare documents in the set to other documents in the set, using **cosine similarity**.\n",
    "\n",
    "Consider two vectors $\\vec{a}$ and $\\vec{b}$.  Using the dot-product, we can determine if they point in the same direction.  \n",
    "\n",
    "\n",
    "$$ cos \\theta = \\dfrac{\\vec{a} \\cdot \\vec{b}}{\\| \\vec{a} \\|  \\cdot  \\|  \\vec{b} \\|  }\n",
    "$$\n",
    "\n",
    "where cosine is the dot/scalar product of two vectors divided by the product of their Euclidean norms.\n",
    "\n",
    " \n",
    "The nice thing about cosine similarity is that it is normalized: no matter what the input vectors are, the output is between 0 and 1. One way to think of this is that cosine similarity is just, um, the cosine function, which has this property (for non-negative  **a**  and  **b** ). Another way to think of it is, to work through the situations of maximum and minimum similarity between two context vectors, starting from the definition above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to convert raw text into tokens as detailed above\n",
    "def preprocessText2Word(raw_text): #tokenize, lowercase, remove stopwords, lemmatize, remove punctuation\n",
    "    tokenizer      = nltk.tokenize.word_tokenize\n",
    "    stop_words     = set(nltk.corpus.stopwords.words('english'))\n",
    "    wnl            = nltk.WordNetLemmatizer()\n",
    "    #-------\n",
    "    tokens         = tokenizer(raw_text)                                     #step 2\n",
    "    tokens         = [ word.lower() for word in tokens ]                     #step 3 make all tokens lowercase\n",
    "    tokens         = [ w for w in tokens if not w in stop_words ]            #step 4 remove stop words\n",
    "    tokens         = [word for word in tokens if word.isalpha()]             #step 5 remove non-alpha characters     \n",
    "    tokens         = [ porter.stem( t ) for t in tokens ]              #step 6 stem or lemmatize\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocs = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in the sky is bright\",\n",
    "\"We can see the shining sun, the bright sun\"\n",
    ")\n",
    "#=======================  \n",
    "# tokenize all 'documents'\n",
    "allDocs_processed=[]\n",
    "for a in allDocs:\n",
    "    tokens = preprocessText2Word(a) #this function does all the heavy-lifting, tokenization, remove stop words\n",
    "    allDocs_processed.append(tokens) #convert from tokens back to text \n",
    "#convert output to tuple (expected by tfidf)    \n",
    "allDocs_processed=tuple(allDocs_processed) #convert it to a tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tfidf to vectorize our data.\n",
    "\n",
    "Using pre-processed, tokenized data, we can vectorize the data by the frequency of these tokens.\n",
    "\n",
    "With our cleaned up text, we can now use it whatever analysis we want. Unfortunately, calculating tf-idf is not available in NLTK but use the one from scikit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================   \n",
    "# since we want to pre-process the data ourself, we initialize with the following inputs\n",
    "tfidf    = TfidfVectorizer(preprocessor=' '.join,lowercase=False)   # initialize the TFIDF vectorizer\n",
    "tfs_vecs = tfidf.fit_transform(allDocs_processed)                                # vectorize on the training data\n",
    "#=======================\n",
    "cosine_similarity(tfs_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also find the similarity of a new vector with our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query        = (\"the very bright sun\")\n",
    "query_tokens = preprocessText2Word(query)\n",
    "query_matrix = tfidf.transform([query_tokens])     # we need the [] to make it a list\n",
    "print(cosine_similarity(query_matrix, tfs_vecs))   # find the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
