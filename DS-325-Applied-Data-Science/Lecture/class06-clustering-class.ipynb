{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science 325\n",
    "\n",
    "## Clustering\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "* Students will learn the motivation for clustering techniques.\n",
    "* Students will be exposed to two algorithmic approaches to clustering: kMeans, DBSCAN\n",
    "* Students will practice the application of these techniques and visualize their results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns; \n",
    "sns.axes_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits, labels_actual = load_digits(return_X_y=True)\n",
    "X_std = StandardScaler().fit_transform(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to help visualize\n",
    "tsne = TSNE(random_state=0, perplexity=30)\n",
    "projected = tsne.fit_transform(X_std)  # project from 64 to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow plot:  deciding number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kMax = 15\n",
    "ks  = np.linspace(2,kMax,kMax-1).astype(int)\n",
    "e_score = np.zeros(ks.shape)\n",
    "for i,k in enumerate(ks):\n",
    "    kmeans = KMeans(n_clusters = k).fit(projected)\n",
    "    e_score[i] = kmeans.inertia_  ## -kmeans.score(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8.1,5),dpi=75)\n",
    "plt.plot(ks,e_score,'o-');\n",
    "plt.xlabel('$k$', fontsize=20)\n",
    "plt.ylabel('Sum of Squared Distances', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is no clear elbow.  This is always best to try first, its quicker than Silhouette and worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette\n",
    "\n",
    "lets try using the silhouette to find the optimal **k**.\n",
    "\n",
    "The silhouette Coefficient = \n",
    "\n",
    "$$\\dfrac{(x-y)}{ max(x,y)}$$\n",
    "\n",
    "* here y is the mean intra cluster distance: mean distance to the other instances in the same cluster. \n",
    "* x depicts mean nearest cluster distance i.e. mean distance to the instances of the next closest cluster.\n",
    "\n",
    "\n",
    "Silhouette index (overall average silhouette) a larger Silhouette value indicates a better quality of a clustering result [Chen et al. 2002]\n",
    "\n",
    "| Silhouette score | note | \n",
    "| --- | --- | \n",
    "| 0.71-1.0  | A strong structure has been found |\n",
    "| 0.51-0.70 | A reasonable structure has been found |\n",
    "| 0.26-0.50 | The structure is weak and could be artificial. Try additional methods of data analysis. |\n",
    "| < 0.25 | No substantial structure has been found |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kMax = 20\n",
    "ks  = np.linspace(2,kMax,kMax-1).astype(int)\n",
    "s_score = np.zeros(ks.shape)\n",
    "for i,k in enumerate(ks):\n",
    "    kmeans = KMeans(n_clusters = k).fit(projected)\n",
    "    #kmeans = KMeans(n_clusters = k).fit(X_std)    \n",
    "    labels = kmeans.labels_\n",
    "    s_score[i] = silhouette_score(projected, labels, metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8.1,5),dpi=75)\n",
    "plt.plot(ks,s_score,'o-');\n",
    "plt.xlabel('$k$', fontsize=20)\n",
    "plt.ylabel('Silhouette Score', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sihouette score is small, but there is a nice local peak around **10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBest = 10\n",
    "kmeans = KMeans(n_clusters = kBest).fit(projected)\n",
    "#kmeans = KMeans(n_clusters = kBest).fit(X_std)\n",
    "labels = kmeans.labels_\n",
    "fig = plt.figure(figsize = (16,5),dpi=75)\n",
    "fig.add_subplot(121)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1],c=labels, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('Spectral', kBest))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - kMeans')\n",
    "plt.colorbar();\n",
    "fig.add_subplot(122)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1],c=labels_actual, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('Spectral', kBest))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - actual')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like kMeans did a great job of clustering the data.  There are some problems (like the kMeans did not distinguish the **d=3** & **d=9** cluster).\n",
    "\n",
    "Lets try with **k=10** instead of 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBest = 8\n",
    "kmeans = KMeans(n_clusters = kBest).fit(projected)\n",
    "labels = kmeans.labels_\n",
    "fig = plt.figure(figsize = (16,5),dpi=75)\n",
    "fig.add_subplot(121)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1],c=labels, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('Spectral', kBest))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - kMeans')\n",
    "plt.colorbar();\n",
    "fig.add_subplot(122)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1],c=labels_actual, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('Spectral', kBest))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - actual')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to know how many clusters we are expecting.  But that is kinda the point here.  If we are using kMeans, then we have to decide how many clusters there are (using Elbow or Silhouette methods).  \n",
    "\n",
    "If we **know** how many clusters there are, the we can use kNN (discussed in lecture) which is a supervised training algorithm.  More on that coming later.  Back to **unsupervised** clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=3, min_samples=10).fit(projected)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "fig = plt.figure(figsize = (16,5),dpi=75)\n",
    "fig.add_subplot(121)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1], c=db.labels_, s=20, alpha=0.5,cmap='Spectral');\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - DBSCAN')\n",
    "plt.colorbar();\n",
    "fig.add_subplot(122)   #top left\n",
    "plt.scatter(projected[:, 0], projected[:, 1],c=labels_actual, edgecolor='none', alpha=0.5,cmap=plt.cm.get_cmap('Spectral', kBest))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.title('labeled - actual')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably better segmentation than the above kMeans.  \n",
    "\n",
    "Are the data from the center cluster (d=0 and d=8) really different clusters?\n",
    "\n",
    "Again, here the problem may not be with kMeans or DBSCAN in segmenting the clusters, but whether these clusters are distinct.  This is always an issuee that needs to be discussed when using unsupervised clustering.\n",
    "\n",
    "Next, lets explore what the DBSCAN parameters do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try three different epsilon values\n",
    "eps_array = np.array([3,5,8,10,15,20])\n",
    "fig, ax = plt.subplots(2, 3,figsize=(15,10),dpi=75)\n",
    "ax = ax.flat\n",
    "for i in range(eps_array.shape[0]):\n",
    "    my_eps = eps_array[i]\n",
    "    db = DBSCAN(eps=my_eps, min_samples=20).fit(projected)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    ax[i].scatter(projected[:, 0], projected[:, 1], c=db.labels_, s=20, alpha=0.5,cmap='Spectral');\n",
    "    ax[i].set_xlabel('component 1')\n",
    "    ax[i].set_ylabel('component 2')\n",
    "    ax[i].set_title('DBSCAN eps = %d'%my_eps)\n",
    "    #plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon plays a big role, and it helps to visualize the data before choosing epsilon.\n",
    "* If epsilon is too small, everything is in its own cluster.  \n",
    "* If epsilon is too large, everything is just one cluster.\n",
    "\n",
    "\n",
    "Now lets look at min_samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try three different min_samples values\n",
    "my_eps = 4\n",
    "min_samp_array = np.array([2,4,8,16,32,64])\n",
    "fig, ax = plt.subplots(2, 3,figsize=(15,10),dpi=75)\n",
    "ax = ax.flat\n",
    "for i in range(min_samp_array.shape[0]):\n",
    "    samp = min_samp_array[i]\n",
    "    db = DBSCAN(eps=my_eps, min_samples=samp).fit(projected)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    ax[i].scatter(projected[:, 0], projected[:, 1], c=db.labels_, s=20, alpha=0.5,cmap='Spectral');\n",
    "    ax[i].set_xlabel('component 1')\n",
    "    ax[i].set_ylabel('component 2')\n",
    "    ax[i].set_title('DBSCAN minSamples = %d'%samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min samples is also crucial to choose wisely.\n",
    "* If min_samples is too small, everything is in its own cluster.  \n",
    "* If min_samples is too large, everything is just one cluster.\n",
    "\n",
    "Kinda the same as epsilon, but for different reason.  \n",
    " \n",
    "#### Setting min_samples larger helps merge **satellite** outliers with a nearby cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
