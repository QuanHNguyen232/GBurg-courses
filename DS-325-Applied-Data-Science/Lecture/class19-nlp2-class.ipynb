{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3Kew4O2g9Gi"
   },
   "source": [
    "# Class - Natural Language Processing 2 : classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guFGIT0zg9Go",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "# Natural Language Toolkit \n",
    "!pip install \"nltk==3.4.5\"\n",
    "import nltk\n",
    "nltk.download('punkt') #word tokenizer\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer\n",
    "import requests #web\n",
    "from collections import Counter #counting words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string \n",
    "import re\n",
    "#-----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns; \n",
    "sns.axes_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "np.random.seed(0)\n",
    "#-----------------------------\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "#from nltk.lm import MLE\n",
    "import time\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc3YjWePg9Gp"
   },
   "source": [
    "## Text classification: Is it Spam?\n",
    "\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "\n",
    "\n",
    "#### Approach\n",
    "* Load the Data\n",
    "* Split/Tokenize words\n",
    "* Lower case\n",
    "* Stem and handle Stop Words\n",
    "* Applying Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BDvgQDXg9Gp"
   },
   "outputs": [],
   "source": [
    "webaddr='http://public.gettysburg.edu/~jpuckett/ds325/data/'\n",
    "df = pd.read_csv(webaddr+\"spam.csv\", encoding = \"latin-1\")\n",
    "df = df[['v1', 'v2']]\n",
    "df = df.rename(columns = {'v1': 'class', 'v2': 'text'})\n",
    "#df.dropna(inplace=True,axis=1)\n",
    "class_names=['ham','spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HczMB0f7g9Gq"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1vq-Mzfg9Gq"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVvy9I34g9Gq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['class'].value_counts())\n",
    "ax =sns.countplot(x=df['class']); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98QYpG4lg9Gq"
   },
   "source": [
    "### Encode the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA7V4t5wg9Gr"
   },
   "outputs": [],
   "source": [
    "df['target'] = df['class'].map( {'spam': 1, 'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwPqt393g9Gr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_count = df.target.value_counts()\n",
    "print('Proportion:', round(target_count[1] / target_count.sum(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPSt72ZYg9Gr"
   },
   "source": [
    "This is the threshold for our classifier.  If our classifier just picked the largest class every time, then it would have **87%** accuracy.  We need to do better than that.  In the following example, we show one strategy on how to mitigate un-balanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUjmiYegg9Gs"
   },
   "source": [
    "### Find the length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANnUDDueg9Gs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['length'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHp0BoH6g9Gs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x='length', hue='class', shade=True, common_norm=False)  \n",
    "plt.xlim(0,220)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK-siJxcg9Gs"
   },
   "source": [
    "It can be seen that ham messages are shorter than spam messages as the distribution of ham and spam message lengths are centered around 30-40 and 155-160 characters, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxy5QYfVg9Gs"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbHfTqy0g9Gt"
   },
   "source": [
    "### classify with Naive Bayes on length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TKKlNocg9Gt"
   },
   "outputs": [],
   "source": [
    "# splitting training data into train and test\n",
    "X = df['length'].values.reshape(-1, 1)\n",
    "Y = df['target']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=.3, random_state=325)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWPQpp4cg9Gt"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO753nxUg9Gt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : {} %\".format(round(acc*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikk6AC0Zg9Gt",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "cmd=ConfusionMatrixDisplay(cm, display_labels=class_names) \n",
    "cmd.plot(ax=ax)\n",
    "plt.title(\"Accuracy : {} %\".format(round(acc*100, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnndsJDqg9Gt"
   },
   "source": [
    "Just using length, we were able to get **87%** accuracy, let's see if we can do better with NLP tools.  \n",
    "\n",
    "* You can see the NB classifier is just choosing everything is **ham**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnj5fi6kg9Gu"
   },
   "source": [
    "### Use Natural Language Processing\n",
    "\n",
    "### define our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRnKI3sHg9Gu"
   },
   "outputs": [],
   "source": [
    "#define function to convert raw text into tokens as detailed above\n",
    "    \n",
    "def cleanText(raw_text): #tokenize, lowercase, remove stopwords, remove punctuation, lemmatize\n",
    "    tokenizer      = nltk.tokenize.word_tokenize\n",
    "    stop_words     = set(nltk.corpus.stopwords.words('english'))\n",
    "    stemmer        = nltk.stem.PorterStemmer()\n",
    "    wnl            = nltk.WordNetLemmatizer()\n",
    "    ##\n",
    "    tokens         = tokenizer(raw_text)                                     #step 1\n",
    "    tokens         = [ word.lower() for word in tokens ]                     #step 2 make all tokens lowercase\n",
    "    tokens         = [ w for w in tokens if not w in stop_words ]            #step 3 remove stop words\n",
    "    tokens         = [word for word in tokens if word.isalpha()]             #step 4 remove non-alpha characters     \n",
    "    tokens         = [ porter.stem( t ) for t in tokens ]              #step 5 stem or lemmatize\n",
    "    text           = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeZZRpMXg9Gu"
   },
   "outputs": [],
   "source": [
    "df['clean']  = df.text.map(lambda x: cleanText(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXMS998yg9Gu"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_ygvCAwg9Gu"
   },
   "outputs": [],
   "source": [
    "# splitting training data into train and test\n",
    "Y = df['target']\n",
    "X = df['clean']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=.3, random_state=325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuEkB7OAg9Gv"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLMQyI52g9Gv"
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j86LAQ6gg9Gv"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly. The text must be converted into numbers—more specifically, vectors of numbers.  We'll use TF-IDF which stands for Term Frequency-Inverse Document Frequency. \n",
    "\n",
    "\n",
    "##### max_df is used for removing terms that appear too frequently\n",
    "* also known as \"corpus-specific stop words\". \n",
    "\n",
    "* **max_df** = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "* **max_df** = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "* The default **max_df=1**, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "\n",
    "##### **min_df** is used for removing terms that appear too infrequently. \n",
    "\n",
    "* **min_df** = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "* **min_df** = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "* The default **min_df=1**, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oElUlZW-g9Gv"
   },
   "outputs": [],
   "source": [
    "tfidf        = TfidfVectorizer(min_df=0.01) #ignore in-frequently used terms\n",
    "tfidf.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qx02QO6zg9Gw"
   },
   "outputs": [],
   "source": [
    "X_train_vec  = tfidf.transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKDhChcsg9Gw"
   },
   "source": [
    "### classify with Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnvYM-_Tg9Gw"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tTbJ8Tfg9Gw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_vec  = tfidf.transform(X_test).toarray()\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : {} %\".format(round(acc*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjmjAOPug9Gw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "cmd=ConfusionMatrixDisplay(cm, display_labels=class_names) \n",
    "cmd.plot(ax=ax)\n",
    "plt.title(\"Accuracy : {} %\".format(round(acc*100, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cCFZ71Yg9Gw"
   },
   "source": [
    "NB is handling this spam filter with over **96%** accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv_sCXNbg9Gx"
   },
   "source": [
    "### Find the Important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGsQbdxng9Gx"
   },
   "source": [
    "How to determine the important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqrjbDTrg9Gx"
   },
   "outputs": [],
   "source": [
    "def getMostImportantFeaturesNB(clf,tfidf,N=20):\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    nclasses      = clf.feature_log_prob_.shape[0]\n",
    "    features      = []\n",
    "    for i in range(nclasses):\n",
    "        feature_prob  = (clf.feature_log_prob_[i]) #class i\n",
    "        indices       = np.argsort(np.abs(feature_prob))[::-1]\n",
    "        features_i    = []\n",
    "        for j in range(N):\n",
    "            features_i.append(feature_names[indices[j]])\n",
    "        features.append(features_i)\n",
    "    for i in range(nclasses):\n",
    "        print(\"class %d important features\"%i)\n",
    "        print(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUisNvuJg9Gx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getMostImportantFeaturesNB(clf,tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T6ahRB8g9Gy"
   },
   "source": [
    "# Example 2: amazon review sentiment\n",
    "\n",
    "This dataset consists of reviews from amazon. The data span a period of 18 years and refer to cell phone and accessories reviewed by users.\n",
    "\n",
    "\n",
    "* [http://snap.stanford.edu/data/web-Amazon.html](http://snap.stanford.edu/data/web-Amazon.html)\n",
    "* J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. RecSys, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvmiuVcVg9Gy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "webaddr='http://public.gettysburg.edu/~jpuckett/ds325/data/'\n",
    "df = pd.read_csv(webaddr+'amazon-cellphone.csv.zip',compression='gzip')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OINK-0dg9Gy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyGJC9Dug9Gy"
   },
   "source": [
    "Drop missing reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR8yWTvAg9Gy"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2HbB7q8g9Gy"
   },
   "source": [
    "Rename our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEvh197ug9Gy"
   },
   "outputs": [],
   "source": [
    "df.rename({'overall': 'class', 'reviewText': 'text'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqvly9mgg9Gz"
   },
   "outputs": [],
   "source": [
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMCI4Lojg9Gz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax =sns.countplot(x=df['class']); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A14FUNBWg9Gz"
   },
   "source": [
    "### Coarse-graining\n",
    "\n",
    "The labels for the reviews are “fine-grained” sentiment labels ranging from 1 to 5: highly negative, negative, neutral, positive, and highly positive.\n",
    "\n",
    "We are tackling a simplified version of this task which frequently appears in the literature: positive/negative\n",
    "binary sentiment classification of sentences, with neutral sentences discarded from the dataset\n",
    "\n",
    "* We remove 3 data as neutral to focus on 'positive' and 'negative' sentiment.\n",
    "* Do we really expect 1-star and 2-star reviews to be very different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b4D7Vo8g9Gz"
   },
   "outputs": [],
   "source": [
    "#remove neutral 3's\n",
    "df = df[df['class'] != 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJIz80bxg9Gz"
   },
   "source": [
    "### encode the review into positive and negative based on rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeUld6eEg9G0"
   },
   "outputs": [],
   "source": [
    "# Encoding 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
    "df['target'] = np.where(df['class'] > 3, 1, 0)\n",
    "class_names=['negative','positive']\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd13WUoCg9G0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax =sns.countplot(x=df['target']); \n",
    "ax.set_xticklabels(class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yp96TJARg9G0"
   },
   "outputs": [],
   "source": [
    "target_count = df.target.value_counts()\n",
    "print('Proportion:', round(target_count[1] / target_count.sum(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GBLOMrtg9G0"
   },
   "source": [
    "### This dataset is very unbalanced.\n",
    "\n",
    "* For instance, if our model predicted every reviewer's response to be **positive**, the accuracy would be **86%**. \n",
    "\n",
    "### Resampling\n",
    "A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).\n",
    "\n",
    "* under-sampling\n",
    "* over-sampling\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png'></img>\n",
    "\n",
    "\n",
    "Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n",
    "\n",
    "Let's implement a basic example, which uses the DataFrame.sample method to get random samples each class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxk06xYRg9G0"
   },
   "source": [
    "## Random under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBC1t1meg9G0"
   },
   "outputs": [],
   "source": [
    "# Divide by class\n",
    "df_class_0    = df[df['target'] == 0] #class with fewer samples\n",
    "df_class_1    = df[df['target'] == 1] #class with more samples\n",
    "count_class_0 = len(df_class_0)\n",
    "count_class_1 = len(df_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mhWF9ahg9G1"
   },
   "outputs": [],
   "source": [
    "df_class_1_under = df_class_1.sample(count_class_0)\n",
    "df_under         = pd.concat([df_class_1_under, df_class_0], axis=0)\n",
    "print('Random under-sampling:')\n",
    "print(df_under.target.value_counts())\n",
    "#plot\n",
    "ax = sns.countplot(x=df_under['target']); \n",
    "ax.set_xticklabels(class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y759ft0Rg9G1"
   },
   "source": [
    "## Random over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ISrtES_g9G1"
   },
   "outputs": [],
   "source": [
    "df_class_0_over = df_class_0.sample(count_class_1, replace=True)\n",
    "df_over         = pd.concat([df_class_0_over, df_class_1], axis=0)\n",
    "print('Random over-sampling:')\n",
    "print(df_over.target.value_counts())\n",
    "#plot\n",
    "ax = sns.countplot(x=df_over['target']); \n",
    "ax.set_xticklabels(class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhzICGS3g9G1"
   },
   "source": [
    "### define our tokenizer, clean the text, prepare for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpTEs18ig9G1"
   },
   "outputs": [],
   "source": [
    "#define function to convert raw text into tokens as detailed above\n",
    "tokenizer      = nltk.tokenize.word_tokenize\n",
    "stop_words     = set(nltk.corpus.stopwords.words('english'))\n",
    "wnl            = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def cleanText(raw_text): #tokenize, lowercase, remove stopwords, removePunctuation, lemmatize\n",
    "    tokenizer      = nltk.tokenize.word_tokenize\n",
    "    stop_words     = set(nltk.corpus.stopwords.words('english'))\n",
    "    wnl            = nltk.WordNetLemmatizer()\n",
    "    #---\n",
    "    tokens         = tokenizer(raw_text)                                #step 1    \n",
    "    tokens         = [ word.lower() for word in tokens ]                #step 2\n",
    "    tokens         = [ w for w in tokens if not w in stop_words ]       #step 3\n",
    "    tokens         = [ w for w in tokens if w.isalpha() ]               #step 4\n",
    "    tokens         = [ wnl.lemmatize ( t ) for t in tokens ]            #step 5\n",
    "    text           = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kryeOG-Zg9G1"
   },
   "source": [
    "### re-check to make sure no NaN values\n",
    "\n",
    "our NLTK tokenizer can't handle nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgLvtFRpg9G1"
   },
   "outputs": [],
   "source": [
    "df_under.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCyailYwg9G2"
   },
   "outputs": [],
   "source": [
    "# cleaning/processing the text can take a few minutes\n",
    "df_under['clean']  = df_under.text.map(lambda x: cleanText(x)) #this may take a few minutes\n",
    "df_under['length'] = df_under['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd1ZS3X-g9G2"
   },
   "outputs": [],
   "source": [
    "df_under.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMLZSvdJg9G2"
   },
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErqD5612g9G2"
   },
   "outputs": [],
   "source": [
    "# splitting training data into train and test\n",
    "Y = df_under['target']\n",
    "X = [d for d in df_under['clean']]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=.3, random_state=325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgOxpoHYg9G2"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s70n-TV-g9G2"
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyXc3QOrg9G2"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly. The text must be converted into numbers—more specifically, vectors of numbers.  We'll use TF-IDF which stands for Term Frequency-Inverse Document Frequency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmbrWcvDg9G3"
   },
   "outputs": [],
   "source": [
    "tfidf        = TfidfVectorizer(min_df=0.001)\n",
    "tfidf.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC4c5yWeg9G3"
   },
   "outputs": [],
   "source": [
    "X_train_vec  = tfidf.transform(X_train).toarray()\n",
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H8LuJIHg9G3"
   },
   "source": [
    "### classify with Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqNVte8ag9G3"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYIoNP-Bg9G3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_vec  = tfidf.transform(X_test).toarray()\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : {} %\".format(round(acc*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I9niGxHg9G3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "cmd=ConfusionMatrixDisplay(cm, display_labels=class_names) \n",
    "cmd.plot(ax=ax)\n",
    "plt.title(\"Accuracy : {} %\".format(round(acc*100, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXWgHu1ug9G3"
   },
   "source": [
    "#### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHvURZdEg9G3"
   },
   "outputs": [],
   "source": [
    "def getMostImportantFeaturesNB(clf,tfidf,N=20):\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    nclasses      = clf.feature_log_prob_.shape[0]\n",
    "    clf.feature_log_prob_.shape\n",
    "    features      = []\n",
    "    for i in range(nclasses):\n",
    "        feature_prob  = (clf.feature_log_prob_[i]) #class i\n",
    "        indices       = np.argsort(np.abs(feature_prob))[::-1]\n",
    "        features_i    = []\n",
    "        for j in range(N):\n",
    "            features_i.append(feature_names[indices[j]])\n",
    "        features.append(features_i)\n",
    "    for i in range(nclasses):\n",
    "        print(\"class %d important features\"%i)\n",
    "        print(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTBzWYmXg9G4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getMostImportantFeaturesNB(clf,tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlJsbGDTg9G4"
   },
   "source": [
    "## Introducing the pipeline\n",
    "\n",
    "### Use Bi-grams with TF-IDF\n",
    "\n",
    "##### max_df is used for removing terms that appear too frequently\n",
    "* also known as \"corpus-specific stop words\". \n",
    "\n",
    "* **max_df** = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "* **max_df** = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "* The default **max_df=1**, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "\n",
    "##### **min_df** is used for removing terms that appear too infrequently. \n",
    "\n",
    "* **min_df** = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "* **min_df** = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "* The default **min_df=1**, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTrkR6nwg9G4"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), #define our vectorizer\n",
    "    ('clf', MultinomialNB()),      #define our classifier\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__min_df': (0,   0.001),\n",
    "    'tfidf__max_df': (0.5, 1.0),\n",
    "    'tfidf__max_features': [1000, None],\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIB8r5pEg9G4"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "grid.fit(X_train,y_train);\n",
    "print(\"GridSearch took %3.2f seconds \" % (time.time() - start_time)) #took about 72s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXBKw5Kqg9G4"
   },
   "outputs": [],
   "source": [
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Params: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w_BDECIg9G4"
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_  #should already be trained/fit\n",
    "#model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99dtykGAg9G5"
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : {} %\".format(round(acc*100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa84tlb_g9G5"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "cmd=ConfusionMatrixDisplay(cm, display_labels=class_names) \n",
    "cmd.plot(ax=ax)\n",
    "plt.title(\"Accuracy : {} %\".format(round(acc*100, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVVKiBxcg9G5"
   },
   "source": [
    "## Application\n",
    "\n",
    "Let's now use our model to make a function that will determine if a review is positive or negative.\n",
    "\n",
    "\n",
    "#### Label probabilities for a sentence\n",
    "The classification is essentially $$P(\\text{class}_1|\\text{review})$$. \n",
    "\n",
    "To find out, we need to use the **.predict_proba** method instead of the usual .predict. \n",
    "\n",
    "Below demonstrates how to find the probability estimates assigned to either label for some reviews I scaped from Amazon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrmJIzCog9G5"
   },
   "outputs": [],
   "source": [
    "def findSentiment(raw_text,tfidf,clf): #tfidf is our vectorizer, clf is our classifier\n",
    "    clean_text = [cleanText(rt) for rt in raw_text] #important to prepare data the same way\n",
    "    tfs_vecs   = tfidf.transform(clean_text) #must just be transform, don't re-fit\n",
    "    tfidf_data = tfs_vecs.toarray()\n",
    "    y_pred     = clf.predict(tfidf_data)\n",
    "    y_class    = ['positive' if y==1 else 'negative' for y in y_pred  ]\n",
    "    y_prob     = clf.predict_proba(tfidf_data)\n",
    "    out        = [(c,round(p.max(),4)) for c,p in zip(y_class,y_prob)]   #get class and probability\n",
    "    for o in out:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPQ3PQXeg9G5"
   },
   "outputs": [],
   "source": [
    "tfidf      = model[0]    #vectorizer can be indexed from the pipeline model\n",
    "clf        = model[1]    #classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8y6jZOUg9G6"
   },
   "outputs": [],
   "source": [
    "#=============================   manually pulled from website\n",
    "review1=\"It fits perfectly with my iPhone 12 and the quality looks good too!\"\n",
    "review2=\"Sturdy case as always. It's not small, and adds a fair bit to the tablet, but it protects extremely well and the stand that is built into the top is extremely handy. Gives you options on what angle to hold the tablet as well as horizontal and vertical positions. The screen protector built in has not affect the touch screen at all either.\"\n",
    "review3=\"I really love it. The dust absorber helps a lot and remove all dust, and the screen protector is super easy to put on without any bubbles. Finally I had a glass-like screen and protect. See the pics attached, I love it and strongly recommend.\"\n",
    "review4=\"One word: impressive. I like the mint green color.I did not attach the strap bc I don’t plan on walking around with an iPad on my shoulder. I think for the price you will not be disappointed.\"\n",
    "goodReviews=[review1,review2,review3,review4]\n",
    "findSentiment(goodReviews,tfidf,clf) #pass our model, vectorizer and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArziWxL7g9G6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#=============================   manually pulled from website   \n",
    "review1=\"This is was a complete waste of money. The protective screen on the cover is super flimsy and wobbles and bubbles when you touch it. The rubber pay off the case doesn't sir on the plastic case right and you can't get at the volume and power button. Threw it in the trash.\"\n",
    "review2=\"It looked good for a few days. But broke in a few days for a simple fall.\"\n",
    "review3=\"The hand strap tore up after a day, terrible quality. Not worth the money.\"\n",
    "review4=\"I believe this case will fit my needs, however when I rec'd the package, I noticed a couple of things were missing which makes me wonder if I rec'd a used item. Still waiting for a reply back. I was told by the manufacture when I reached out to them to contact the seller. So not to sure about the seller not taking responsibility with their product? I'm sure this will get worked out, but have to be honest with my review.\"\n",
    "badReviews=[review1,review2,review3,review4]\n",
    "findSentiment(badReviews,tfidf,clf) #pass our model, vectorizer and classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUibjApgg9G6"
   },
   "source": [
    "# Maximal likelihood estimates and text generation\n",
    "\n",
    "We will use MLE to generate fake text.\n",
    "\n",
    "We don't want to used the cleaned text above that was 'stemmed' or 'lemmatized' and we'd like to keep stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62dBWJfng9G6"
   },
   "outputs": [],
   "source": [
    "# lets just clean up the raw reviews by remove special characters and making every lower case.\n",
    "X_mle = [re.sub(\"[()!#]\", \" \", d.lower() ) for d in df_under['text']]\n",
    "X_mle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LODX6g1tg9G6"
   },
   "outputs": [],
   "source": [
    "#tokenize the words of each review\n",
    "tokenized_text = [list(word_tokenize(x)) for x in X_mle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khcLYlFEg9G6"
   },
   "outputs": [],
   "source": [
    "n=2 #bigrams\n",
    "train_data, padded_vocab = nltk.lm.preprocessing.padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGdPfEmRg9G7"
   },
   "source": [
    "Construct model from the n-gram padded vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfnIVNCEg9G7"
   },
   "outputs": [],
   "source": [
    "model = nltk.lm.MLE(n)\n",
    "# fit on padded vocab that the model know the new tokens added to vocab (<s>, </s>, UNK etc)\n",
    "model.fit(train_data, padded_vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD7t7HQQg9G7"
   },
   "outputs": [],
   "source": [
    "model.counts['one'] # i.e. Count('was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMR8HZWcg9G7"
   },
   "outputs": [],
   "source": [
    "model.score(\"one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70pN0ZZ3g9G7"
   },
   "outputs": [],
   "source": [
    "model.counts[['one']]['click'] # i.e. Count('one'|'click')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nafKdNaAg9G7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Smj_G_ng9G7"
   },
   "source": [
    "Generate a sentence based on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26jkD5B-g9G8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_sent(model, num_words=20, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMIKnuDFg9G8"
   },
   "source": [
    "### Advantages of N-grams\n",
    "1. It gives insight at different levels. (bigram, trigram, N-gram).\n",
    "2. Simple and conceptually easy to understand.\n",
    "\n",
    "### Disadvantages of N-grams\n",
    "1. We may need to use stop words to avoid any noise in results.\n",
    "2. A count may not necessarily indicate importance to text or entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWPF3AEMg9G8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VbHfTqy0g9Gt",
    "CXMS998yg9Gu",
    "j86LAQ6gg9Gv",
    "DKDhChcsg9Gw",
    "Dv_sCXNbg9Gx",
    "1GBLOMrtg9G0",
    "5H8LuJIHg9G3",
    "xXWgHu1ug9G3"
   ],
   "name": "class19-nlp2-class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
