{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class - Training NNs\n",
    "\n",
    "#### Motivation. \n",
    "This is designed to familiarize you with using ML frameworks for deep learning. In particular, there are the following learning goals:\n",
    "\n",
    "* Practice writing code that implements neural network training with tensorflow\n",
    "* Practice replicating results from previously published work.\n",
    "* Evaluate multiple optimization methds for training.\n",
    "* Explore hand-tuning for hyperparameter optimization.\n",
    "* It builds on the previous assignment but requires a higher level of mastery of deep learning theory and programming/engineering skills. In particular, you will experience training a much deeper network on a large-scale dataset. You will encounter practical issues that help you consolidate learning.\n",
    "\n",
    "In the previous assignment, you tackled the image classification problem in MNIST, here you will explore model architecture for Densely Connected Neural Network to improve the image classification performance.\n",
    "\n",
    "\n",
    "### Fashion MNIST\n",
    "\n",
    "MNIST has been over-explored, state-of-the-art on MNIST doesn’t make much sense with over 99% already achieved. Fashion-MNIST is a dataset of Zalando's serving as a 'drop-in' replacement for MNIST for benchmarking machine learning algorithms. The dataset is the same size: consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "Your trained network will take as input a feature vector of dimension 784 (corresponding to the pixel values of 28×28 images), each an integer from 0–1. The class labels are in the following table:\n",
    "\n",
    "| Label Value |\tMeaning |\n",
    "| - | - |\n",
    "|0 |\tT-shirt/top |\n",
    "|1 |\tTrouser |\n",
    "|2 |\tPullover |\n",
    "|3 |\tDress |\n",
    "|4 |\tCoat |\n",
    "|5 |\tSandal |\n",
    "|6 |\tShirt |\n",
    "|7 |\tSneaker |\n",
    "|8 |\tBag |\n",
    "|9 |\tAnkle boot |\n",
    "\n",
    "#### Benchmarks of Fashion MNIST for various algorithms\n",
    "* Human accuracy is 0.835\n",
    "\n",
    "* Xiao, Han, Kashif Rasul, and Roland Vollgraf. \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.\" arXiv preprint arXiv:1708.07747 (2017).\n",
    "[https://arxiv.org/pdf/1708.07747.pdf](https://arxiv.org/pdf/1708.07747.pdf)\n",
    "* Benchmarks\n",
    "[http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "plt.rcParams[\"figure.figsize\"] = [9.708,6]\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "# !pip install tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "We will fix the input shape (to how TF prefers), rescale the data between 0 and 1, and encode the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (X_train.shape[1],X_train.shape[2])\n",
    "#normalize the data between 0-1\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test  = X_test.astype( 'float32') / 255\n",
    "#Reshape To Match The Keras's Expectations\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, input_shape[0], input_shape[1])\n",
    "X_test  = X_test.reshape( X_test.shape[0],  1, input_shape[0], input_shape[1])\n",
    "#one hot encoding\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test  = tf.keras.utils.to_categorical(y_test,  num_classes)\n",
    "#==============\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0],  'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the images first\n",
    "fig=plt.figure()\n",
    "ncols,nrows = len(class_names), 4\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols,figsize=(12,5))\n",
    "print(axs.shape)\n",
    "for i in range(ncols):\n",
    "    inn = y_train==i\n",
    "    Xi  = X_train[inn]\n",
    "    for j in range(nrows):\n",
    "        imgi = Xi[j].reshape((input_shape[0],input_shape[1]))\n",
    "        axs[j,i].imshow(imgi,interpolation='nearest',cmap=plt.cm.gray)\n",
    "        axs[j,i].axis('off')\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network\n",
    "Build and compile a basic model.\n",
    "\n",
    "Layers for our Network.\n",
    "\n",
    "* **Input layer** - size 784 \n",
    "    * flatten the input image (28x28).\n",
    "* **Hidden layer** - size 128\n",
    "    * Dense (fully connected) network from input layer to these 128 neuron hidden layer.\n",
    "* **Dropout** - 0.2\n",
    "    * randomly sets 20% input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. \n",
    "* **Output layer** - size 10\n",
    "    * Dense layer (fully connected back to the 128 neuron hidden layer). The 10 is the number of classes.  Given an input image, our network should **light** up the corresponding neuron of our target.\n",
    "* **Softmax activation** - convert our output into a probability for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1a: 1-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep these constant\n",
    "epochs    = 30\n",
    "batch_size= 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1         = 100                                  # setting my number of neurons\n",
    "tf.random.set_seed(0)                             # set our initial seed\n",
    "model1 = tf.keras.models.Sequential([              # model type\n",
    "  tf.keras.layers.Flatten(input_shape=X_train[1].shape),  # input layer\n",
    "  tf.keras.layers.Dense(d1, activation='relu'),  # hidden layer\n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(10),                      # output to each class, could just stop here\n",
    "  tf.keras.layers.Softmax()                       # convert to probability\n",
    "])\n",
    "#define our optimizer\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name='SGD')\n",
    "#\n",
    "model1.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy', #need to define our loss function\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = tf.timestamp()\n",
    "history1 = model1.fit(X_train, Y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split = 0.2) # Store Data for evaluation in history\n",
    "total_time = tf.timestamp() - tstart\n",
    "print(\"total time %3.3f seconds\"%total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use this a lot, so lets make a function\n",
    "def printAccuracy(history,results_test):\n",
    "    print(\"train loss %.5f \\t train acc: %.5f\"%(history.history['loss'][-1],history.history['accuracy'][-1]))\n",
    "    print(\"valid loss %.5f \\t valid acc: %.5f\"%(history.history['val_loss'][-1],history.history['val_accuracy'][-1]))\n",
    "    print(\"test loss  %.5f \\t test acc:  %.5f\"%(results_test[0],results_test[1]))\n",
    "#we will do this a lot, so lets make a function for this\n",
    "def plot_result(history,results_test):\n",
    "    # Get training and validation histories\n",
    "    training_acc = history.history['accuracy']\n",
    "    val_acc      = history.history['val_accuracy']\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_acc) + 1)\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_acc, 'b-o',label='Training')\n",
    "    plt.plot(epoch_count, val_acc, 'r--',label='Validation')\n",
    "    plt.plot(epoch_count, results_test[1]*np.ones(len(epoch_count)),'k--',label='Test')\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===    \n",
    "results_test = model1.evaluate(X_test, Y_test, batch_size=128,verbose=0)    \n",
    "printAccuracy(history1,results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training and test accuracy per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(history1,results_test)   \n",
    "plt.title(\"MNIST 1-hidden layer, d=%d, in %3.2f s\"%(d1,total_time)) #overwrite the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark, test accuracy 87.1%\n",
    "That our simple 1-hidden layer network for Fashion MNIST, with an accuracy of 87.1%, not bad for a first try.  We can see that we are not stopping too soon, since the Validation loss has leveled off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1b: 2-layer network: Is a deeper neural network more accurate?\n",
    "Here we’ll combine all our steps into the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d1         = 100    # setting my number of neurons\n",
    "d2         = 100\n",
    "#=====\n",
    "tf.random.set_seed(0)                             # set our initial seed\n",
    "model2 = tf.keras.models.Sequential([              # model type\n",
    "  tf.keras.layers.Flatten(input_shape=X_train[1].shape),  # input layer\n",
    "  tf.keras.layers.Dense(d1, activation='relu'),  # hidden layer\n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(d2, activation='relu'),  # hidden layer    \n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(10),                      # output to each class, could just stop here\n",
    "  tf.keras.layers.Softmax()                       # convert to probability\n",
    "])\n",
    "#define our optimizer\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name='SGD')\n",
    "#\n",
    "model2.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy', #need to define our loss function\n",
    "              metrics=['accuracy'])\n",
    "#====\n",
    "tstart   = tf.timestamp()\n",
    "history2 = model2.fit(X_train, Y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split = 0.2) # Store Data for evaluation in history\n",
    "total_time = tf.timestamp() - tstart\n",
    "print(\"total time %3.3f seconds\"%total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===    \n",
    "results_test = model2.evaluate(X_test, Y_test, batch_size=128,verbose=0)    \n",
    "printAccuracy(history2,results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot training and test accuracy per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_result(history2,results_test)   \n",
    "plt.title(\"MNIST 2-hidden layer, d=(%d,%d) in %3.2f s\"%(d1,d2,total_time)) #overwrite the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark, test accuracy 87.7%\n",
    "Our first 2-hidden layer network for Fashion MNIST did not gain much of an improvement over our 1 hidden layer network.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1c - 3 hidden layers - go big or go home\n",
    "\n",
    "Let’s build a 5-layer network (3-hidden layers), keeping the same activation functions, shapes and settings, so the only difference is the depth of the network. Here we’ll combine all our steps into the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1         = 100    # setting my number of neurons\n",
    "d2         = 100\n",
    "d3         = 100\n",
    "#=====\n",
    "tf.random.set_seed(0)                             # set our initial seed\n",
    "model3 = tf.keras.models.Sequential([              # model type\n",
    "  tf.keras.layers.Flatten(input_shape=X_train[1].shape),  # input layer\n",
    "  tf.keras.layers.Dense(d1, activation='relu'),  # hidden layer\n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(d2, activation='relu'),  # hidden layer    \n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(d3, activation='relu'),  # hidden layer    \n",
    "  tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "  tf.keras.layers.Dense(10),                      # output to each class, could just stop here\n",
    "  tf.keras.layers.Softmax()                       # convert to probability\n",
    "])\n",
    "#define our optimizer\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name='SGD')\n",
    "#\n",
    "model3.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy', #need to define our loss function\n",
    "              metrics=['accuracy'])\n",
    "#====\n",
    "tstart   = tf.timestamp()\n",
    "history3 = model3.fit(X_train, Y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split = 0.2) # Store Data for evaluation in history\n",
    "total_time = tf.timestamp() - tstart\n",
    "print(\"total time %3.3f seconds\"%total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===    \n",
    "results_test = model3.evaluate(X_test, Y_test, batch_size=128,verbose=0)    \n",
    "printAccuracy(history3,results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot training and test accuracy per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_result(history3,results_test)   \n",
    "plt.title(\"MNIST 3-hidden layer, d=(%d,%d,%d) in %3.2f s\"%(d1,d2,d3,total_time)) #overwrite the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all 3 models, the general trend we notice as the epochs increase with the training data set is that the accuracy approaches 1 (loss is decreasing down to 0), representing the ‘perfect score’. This is a sign of over-fitting, which is the motivation behind validating the model. This is a well know problem in machine learning, called Overfitting — when the model adapts too well to a specific dataset and thus does not generalize well on new information. There are many ways to correct for this, called Regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc1 = history1.history['val_accuracy']\n",
    "val_acc2 = history2.history['val_accuracy']\n",
    "val_acc3 = history3.history['val_accuracy']\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(val_acc1) + 1)\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, val_acc1, label='1-hidden layer')\n",
    "plt.plot(epoch_count, val_acc2, label='2-hidden layer')\n",
    "plt.plot(epoch_count, val_acc3, label='3-hidden layer')\n",
    "plt.legend()\n",
    "plt.title(\"Fashion MNIST\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the 2-hidden layer marginally performs better for the 30 epochs, so we'll stick with that for the rest of our investigation.\n",
    "#### Note: the 3-hidden layer network seems to still be increasing after 30 epochs (its still learning).  Come back to this for hw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - what effect does batch size have on accuracy?\n",
    "\n",
    "Lets try to see how mini-batch size changes our results.\n",
    "\n",
    "In this part, we will explore how changing the minibatch size hyperparameter $B$. Suppose that we were to decrease this hyperparameter to \n",
    "* $B=100$\n",
    "* $B=200$\n",
    "* $B=400$\n",
    "* $B=800$\n",
    "* $B=1600$\n",
    "* $B=3200$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs        = 30\n",
    "hiddenNeurons = 100\n",
    "Bsizes        = [100,200,400,800,1600,3200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we run almost the same model, and only change the batch_size, we can put everything into a function, and just run it and just change one model parameter.  This technique is really helpful and can save you some trouble on your hw.\n",
    "\n",
    "We can output just what we want too, the validation accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNNModel_BatchSize(B):\n",
    "    tf.random.set_seed(0)                             # set our initial seed\n",
    "    modelB = tf.keras.models.Sequential([             # model type\n",
    "      tf.keras.layers.Flatten(input_shape=X_train[1].shape),  # input layer\n",
    "      tf.keras.layers.Dense(hiddenNeurons, activation='relu'),   # hidden layer\n",
    "      tf.keras.layers.Dropout(0.2),                   # Dropout helps reduce overfitting \n",
    "      tf.keras.layers.Dense(10),                      # output to each class, could just stop here\n",
    "      tf.keras.layers.Softmax()                       # convert to probability\n",
    "    ])\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name='SGD')\n",
    "    modelB.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',    #need to define our loss function\n",
    "                  metrics=['accuracy'])\n",
    "    history = modelB.fit(X_train, Y_train, verbose=0,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=B,\n",
    "                        validation_split = 0.2) \n",
    "    return history.history['val_accuracy']            #all we want is the validation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the model for each of the batch sizes we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valAccuracy = []   \n",
    "for B in Bsizes:\n",
    "    print(\"Starting batchsize %d\"%B,end=' ');\n",
    "    tstart = tf.timestamp()\n",
    "    valAcc_B = runNNModel_BatchSize(B)\n",
    "    valAccuracy.append(valAcc_B)\n",
    "    total_time = tf.timestamp() - tstart\n",
    "    print(\"total time %3.3f seconds\"%total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for v,B in zip(valAccuracy,Bsizes):   \n",
    "    epoch_count = range(1, len(v) + 1)               # Create count of the number of epochs\n",
    "    plt.plot(epoch_count, v,label='B=%d'%B)\n",
    "#===========    \n",
    "plt.legend()\n",
    "plt.title(\"Fashion MNIST\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
